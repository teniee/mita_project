name: MITA Performance Tests and Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC for continuous performance monitoring
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - unit_performance
        - load_tests
        - security_impact
        - memory_tests
        - database_performance
      environment:
        description: 'Environment to test against'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production_clone
        - local

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  unit-performance-tests:
    name: Unit Performance Tests
    runs-on: ubuntu-latest
    if: contains(fromJson('["all", "unit_performance"]'), github.event.inputs.test_type) || github.event.inputs.test_type == ''
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: mita_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client redis-tools

    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r app/tests/performance/locustfiles/requirements.txt
        pip install pytest-benchmark pytest-xdist pytest-cov

    - name: Set up environment variables
      run: |
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/mita_test" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
        echo "ENVIRONMENT=testing" >> $GITHUB_ENV
        echo "JWT_SECRET=test-jwt-secret-for-ci" >> $GITHUB_ENV
        echo "SECRET_KEY=test-secret-key-for-ci" >> $GITHUB_ENV

    - name: Run database migrations
      run: |
        cd app
        python -m alembic upgrade head

    - name: Run Income Classification Performance Tests
      run: |
        cd app
        python -m pytest tests/performance/test_income_classification_performance.py \
          -v -s --tb=short \
          --benchmark-only \
          --benchmark-json=../performance_reports/income_classification_benchmark.json
      continue-on-error: true

    - name: Run Authentication Performance Tests
      run: |
        cd app
        python -m pytest tests/performance/test_authentication_performance.py \
          -v -s --tb=short \
          --benchmark-only \
          --benchmark-json=../performance_reports/auth_benchmark.json
      continue-on-error: true

    - name: Run Security Performance Impact Tests
      run: |
        cd app
        python -m pytest tests/performance/test_security_performance_impact.py \
          -v -s --tb=short \
          --benchmark-only \
          --benchmark-json=../performance_reports/security_benchmark.json
      continue-on-error: true

    - name: Run Memory and Resource Tests
      run: |
        cd app
        python -m pytest tests/performance/test_memory_resource_monitoring.py \
          -v -s --tb=short \
          --benchmark-only \
          --benchmark-json=../performance_reports/memory_benchmark.json
      continue-on-error: true

    - name: Generate Performance Report
      run: |
        python scripts/generate_performance_report.py \
          --input-dir performance_reports \
          --output performance_summary.json
      continue-on-error: true

    - name: Upload Performance Reports
      uses: actions/upload-artifact@v3
      with:
        name: unit-performance-reports
        path: |
          performance_reports/
          performance_summary.json
        retention-days: 30

    - name: Comment Performance Results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const report = JSON.parse(fs.readFileSync('performance_summary.json', 'utf8'));
            
            const comment = `## üöÄ Performance Test Results
            
            ### Income Classification Performance
            - **Target**: 0.08ms per classification
            - **Actual**: ${report.income_classification?.mean_ms || 'N/A'}ms
            - **Status**: ${report.income_classification?.passed ? '‚úÖ PASS' : '‚ùå FAIL'}
            
            ### Authentication Performance
            - **Login Target**: 200ms
            - **Login Actual**: ${report.authentication?.login_ms || 'N/A'}ms
            - **Token Validation**: ${report.authentication?.token_validation_ms || 'N/A'}ms
            
            ### Security Impact
            - **Rate Limiter Overhead**: ${report.security?.rate_limiter_overhead_ms || 'N/A'}ms
            - **Token Blacklist Overhead**: ${report.security?.blacklist_overhead_ms || 'N/A'}ms
            
            ### Memory Usage
            - **Memory Leaks**: ${report.memory?.leaks_detected ? '‚ö†Ô∏è DETECTED' : '‚úÖ NONE'}
            - **Peak Memory**: ${report.memory?.peak_memory_mb || 'N/A'}MB
            
            **Overall Status**: ${report.overall_status || 'UNKNOWN'}
            
            <details>
            <summary>Performance Recommendations</summary>
            
            ${report.recommendations?.map(r => `- ${r}`).join('\n') || 'No specific recommendations'}
            </details>`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not post performance results:', error);
          }

  load-tests:
    name: Load Testing with Locust
    runs-on: ubuntu-latest
    if: contains(fromJson('["all", "load_tests"]'), github.event.inputs.test_type) || github.event.inputs.test_type == ''
    
    strategy:
      matrix:
        load_scenario:
          - smoke_test
          - baseline_performance
          - standard_load
          - auth_load
          - financial_focus

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: mita_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r app/tests/performance/locustfiles/requirements.txt

    - name: Set up environment
      run: |
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/mita_test" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
        echo "ENVIRONMENT=testing" >> $GITHUB_ENV
        echo "JWT_SECRET=test-jwt-secret-for-ci" >> $GITHUB_ENV
        echo "SECRET_KEY=test-secret-key-for-ci" >> $GITHUB_ENV

    - name: Start MITA API Server
      run: |
        cd app
        python -m alembic upgrade head
        python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10
        # Verify server is running
        curl -f http://localhost:8000/health || exit 1

    - name: Run Load Test - ${{ matrix.load_scenario }}
      run: |
        cd app/tests/performance/locustfiles
        python run_load_tests.py \
          --host http://localhost:8000 \
          --test ${{ matrix.load_scenario }}
      timeout-minutes: 30

    - name: Upload Load Test Reports
      uses: actions/upload-artifact@v3
      with:
        name: load-test-reports-${{ matrix.load_scenario }}
        path: app/tests/performance/locustfiles/load_test_reports/
        retention-days: 30

  security-performance-tests:
    name: Security Features Performance Impact
    runs-on: ubuntu-latest
    if: contains(fromJson('["all", "security_impact"]'), github.event.inputs.test_type) || github.event.inputs.test_type == ''

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark

    - name: Set up environment
      run: |
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
        echo "ENVIRONMENT=testing" >> $GITHUB_ENV
        echo "JWT_SECRET=test-jwt-secret-for-ci" >> $GITHUB_ENV

    - name: Run Security Performance Tests
      run: |
        cd app
        python -m pytest tests/performance/test_security_performance_impact.py \
          -v -s --tb=short \
          --benchmark-json=../security_performance_report.json

    - name: Analyze Security Performance Impact
      run: |
        python scripts/analyze_security_performance.py \
          --report security_performance_report.json \
          --threshold-file security_thresholds.json

    - name: Upload Security Performance Report
      uses: actions/upload-artifact@v3
      with:
        name: security-performance-report
        path: security_performance_report.json

  memory-tests:
    name: Memory and Resource Usage Tests
    runs-on: ubuntu-latest
    if: contains(fromJson('["all", "memory_tests"]'), github.event.inputs.test_type) || github.event.inputs.test_type == ''

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install memory-profiler psutil

    - name: Run Memory Leak Detection
      run: |
        cd app
        python -m pytest tests/performance/test_memory_resource_monitoring.py \
          -v -s --tb=short \
          -k "memory_leaks" \
          --json-report --json-report-file=../memory_leak_report.json

    - name: Run Long-Running Stability Test
      run: |
        cd app
        python -m pytest tests/performance/test_memory_resource_monitoring.py \
          -v -s --tb=short \
          -k "long_running_stability" \
          --json-report --json-report-file=../stability_report.json
      timeout-minutes: 15

    - name: Generate Memory Report
      run: |
        python scripts/generate_memory_report.py \
          --leak-report memory_leak_report.json \
          --stability-report stability_report.json \
          --output memory_analysis.json

    - name: Upload Memory Reports
      uses: actions/upload-artifact@v3
      with:
        name: memory-reports
        path: |
          memory_leak_report.json
          stability_report.json
          memory_analysis.json

  database-performance-tests:
    name: Database Performance Tests
    runs-on: ubuntu-latest
    if: contains(fromJson('["all", "database_performance"]'), github.event.inputs.test_type) || github.event.inputs.test_type == ''

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: test_user
          POSTGRES_DB: mita_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark sqlalchemy-utils

    - name: Set up database
      run: |
        echo "DATABASE_URL=postgresql://test_user:test_password@localhost:5432/mita_test" >> $GITHUB_ENV
        cd app
        python -m alembic upgrade head

    - name: Run Database Performance Tests
      run: |
        cd app
        python -m pytest tests/performance/test_database_performance.py \
          -v -s --tb=short \
          --benchmark-json=../database_performance_report.json
      timeout-minutes: 20

    - name: Analyze Database Performance
      run: |
        python scripts/analyze_database_performance.py \
          --report database_performance_report.json \
          --thresholds database_performance_thresholds.json

    - name: Upload Database Performance Report
      uses: actions/upload-artifact@v3
      with:
        name: database-performance-report
        path: database_performance_report.json

  performance-regression-detection:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [unit-performance-tests, load-tests, security-performance-tests, memory-tests]
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install pandas numpy matplotlib scipy

    - name: Download all performance reports
      uses: actions/download-artifact@v3
      with:
        path: all_performance_reports/

    - name: Run Regression Detection Analysis
      run: |
        python scripts/detect_performance_regression.py \
          --reports-dir all_performance_reports/ \
          --baseline-branch main \
          --current-branch ${{ github.head_ref || github.ref_name }} \
          --output regression_report.json
      continue-on-error: true

    - name: Generate Performance Dashboard Data
      run: |
        python scripts/generate_performance_dashboard.py \
          --reports-dir all_performance_reports/ \
          --output dashboard_data.json

    - name: Upload Regression Analysis
      uses: actions/upload-artifact@v3
      with:
        name: performance-regression-analysis
        path: |
          regression_report.json
          dashboard_data.json

    - name: Comment Regression Results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const regression = JSON.parse(fs.readFileSync('regression_report.json', 'utf8'));
            
            if (regression.regressions_detected > 0) {
              const comment = `## ‚ö†Ô∏è Performance Regression Detected
              
              **Regressions Found**: ${regression.regressions_detected}
              
              ### Critical Regressions:
              ${regression.critical_regressions?.map(r => 
                `- **${r.test}**: ${r.change}% slower (${r.current}ms vs ${r.baseline}ms)`
              ).join('\n') || 'None'}
              
              ### All Regressions:
              ${regression.all_regressions?.map(r => 
                `- ${r.test}: ${r.change}% change`
              ).join('\n') || 'None'}
              
              Please review and address these performance regressions before merging.`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } else {
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## ‚úÖ No Performance Regressions Detected\n\nAll performance metrics are within acceptable ranges.'
              });
            }
          } catch (error) {
            console.log('Could not analyze regression results:', error);
          }

  performance-monitoring-dashboard:
    name: Update Performance Monitoring Dashboard
    runs-on: ubuntu-latest
    needs: [performance-regression-detection]
    if: github.ref == 'refs/heads/main' && always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download performance reports
      uses: actions/download-artifact@v3
      with:
        name: performance-regression-analysis

    - name: Update Performance Dashboard
      run: |
        # This would integrate with your monitoring dashboard
        # (Grafana, DataDog, custom dashboard, etc.)
        echo "Updating performance dashboard with latest metrics..."
        # python scripts/update_dashboard.py --data dashboard_data.json

    - name: Store Performance Baseline
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        # Store current performance metrics as baseline for future comparisons
        python scripts/store_performance_baseline.py \
          --commit-sha ${{ github.sha }} \
          --data dashboard_data.json \
          --storage-backend github-releases  # or s3, gcs, etc.

  notify-performance-issues:
    name: Notify Performance Issues
    runs-on: ubuntu-latest
    needs: [performance-regression-detection]
    if: failure() || (github.event_name == 'schedule' && needs.performance-regression-detection.result == 'failure')

    steps:
    - name: Send Slack Notification
      if: env.SLACK_WEBHOOK_URL != ''
      run: |
        curl -X POST -H 'Content-type: application/json' \
          --data '{"text":"üö® MITA Performance Tests Failed on ${{ github.ref_name }}"}' \
          ${{ secrets.SLACK_WEBHOOK_URL }}

    - name: Create GitHub Issue for Performance Regression
      if: github.event_name == 'schedule'
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Performance Regression Detected - ${new Date().toISOString().split('T')[0]}`,
            body: `Automated performance monitoring has detected regressions.
            
            **Run**: ${{ github.run_id }}
            **Commit**: ${{ github.sha }}
            **Branch**: ${{ github.ref_name }}
            
            Please review the performance test results and address any critical issues.`,
            labels: ['performance', 'regression', 'critical']
          });