"""
MITA Security Features Performance Impact Tests
Critical assessment of performance impact from security features including rate limiting,
token blacklist, comprehensive auth security, and audit logging.
"""

import pytest
import time
import asyncio
import statistics
import psutil
from typing import Dict, List, Any, Optional
from unittest.mock import patch, MagicMock, AsyncMock
from dataclasses import dataclass
from datetime import datetime, timedelta
import threading

# Import security components
from app.core.security import AdvancedRateLimiter, SecurityConfig, get_security_health_status
from app.services.token_security_service import TokenSecurityService
from app.core.audit_logging import log_security_event, log_financial_operation
from app.middleware.comprehensive_rate_limiter import ComprehensiveRateLimitMiddleware
from app.services.auth_jwt_service import create_access_token, verify_token
from app.core.rate_limiter import RateLimiter


@dataclass
class SecurityPerformanceBenchmark:
    """Security feature performance benchmark results"""
    feature: str
    operation: str
    baseline_ms: float
    with_security_ms: float
    overhead_ms: float
    overhead_percent: float
    throughput_impact_percent: float
    memory_impact_mb: float
    passed: bool


class SecurityPerformanceTests:
    """
    Critical performance impact assessment for security features.
    Security must not significantly degrade user experience or system performance.
    """
    
    # Maximum acceptable security overhead thresholds
    MAX_RATE_LIMITER_OVERHEAD_MS = 5.0      # Rate limiting should be very fast
    MAX_TOKEN_BLACKLIST_OVERHEAD_MS = 10.0   # Token blacklist checks
    MAX_AUDIT_LOGGING_OVERHEAD_MS = 2.0      # Audit logging should be minimal
    MAX_AUTH_SECURITY_OVERHEAD_MS = 15.0     # Comprehensive auth security
    MAX_SECURITY_MIDDLEWARE_OVERHEAD_MS = 8.0 # Middleware processing
    
    # Acceptable performance degradation percentages
    MAX_THROUGHPUT_DEGRADATION_PERCENT = 15.0  # Max 15% throughput loss
    MAX_RESPONSE_TIME_INCREASE_PERCENT = 20.0  # Max 20% response time increase
    
    # Test parameters
    PERFORMANCE_ITERATIONS = 1000
    CONCURRENT_TEST_USERS = 50
    
    @pytest.fixture
    def mock_redis(self):
        """Mock Redis client for security features"""
        redis_mock = MagicMock()
        redis_mock.ping.return_value = True
        redis_mock.pipeline.return_value = redis_mock
        redis_mock.execute.return_value = [1, 300]  # Count, TTL
        redis_mock.zremrangebyscore.return_value = 0
        redis_mock.zadd.return_value = 1
        redis_mock.zcard.return_value = 1
        redis_mock.expire.return_value = True
        redis_mock.sismember.return_value = False  # Token not blacklisted
        redis_mock.sadd.return_value = 1
        redis_mock.hget.return_value = None
        redis_mock.hset.return_value = True
        redis_mock.incr.return_value = 1
        redis_mock.setex.return_value = True
        redis_mock.get.return_value = "0"  # No violations
        return redis_mock
    
    @pytest.fixture
    def mock_request(self):
        """Mock FastAPI request object"""
        from fastapi import Request
        from unittest.mock import Mock
        
        request = Mock(spec=Request)
        request.client.host = "192.168.1.100"
        request.headers = {
            'User-Agent': 'MITA-Test/1.0',
            'X-Forwarded-For': '10.0.0.1',
            'Authorization': f'Bearer {create_access_token({"sub": "12345"})}'
        }
        request.url.path = "/api/test"
        request.method = "GET"
        request.body = lambda: b'{"test": "data"}'
        return request
    
    def measure_operation_performance(
        self, 
        operation_func, 
        iterations: int = None,
        measure_memory: bool = True
    ) -> Dict[str, float]:
        """
        Measure operation performance with memory tracking.
        """
        iterations = iterations or self.PERFORMANCE_ITERATIONS
        times = []
        process = psutil.Process() if measure_memory else None
        
        # Baseline memory
        baseline_memory = process.memory_info().rss / 1024 / 1024 if process else 0
        
        # Warmup
        for _ in range(10):
            try:
                operation_func()
            except:
                pass
        
        # Measure
        for _ in range(iterations):
            start_time = time.perf_counter()
            try:
                operation_func()
            except Exception:
                pass  # Some operations might fail in mocked environment
            end_time = time.perf_counter()
            times.append((end_time - start_time) * 1000)
        
        # Final memory
        final_memory = process.memory_info().rss / 1024 / 1024 if process else 0
        
        if not times:
            return {"error": "No successful operations"}
        
        return {
            "mean_ms": statistics.mean(times),
            "median_ms": statistics.median(times),
            "std_ms": statistics.stdev(times) if len(times) > 1 else 0,
            "min_ms": min(times),
            "max_ms": max(times),
            "p95_ms": self._percentile(times, 95),
            "p99_ms": self._percentile(times, 99),
            "samples": len(times),
            "memory_growth_mb": final_memory - baseline_memory,
            "throughput_ops_per_sec": iterations / (sum(times) / 1000) if sum(times) > 0 else 0
        }
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """Calculate percentile"""
        sorted_data = sorted(data)
        k = (len(sorted_data) - 1) * (percentile / 100.0)
        f = int(k)
        c = int(k) + 1
        if f == c:
            return sorted_data[f]
        d0 = sorted_data[f] * (c - k)
        d1 = sorted_data[c] * (k - f)
        return d0 + d1
    
    def test_rate_limiter_performance_impact(self, mock_redis, mock_request):
        """
        Test performance impact of rate limiting on request processing.
        Rate limiting is called on every API request, so must be very fast.
        """
        with patch('app.core.security.redis_client', mock_redis):
            rate_limiter = AdvancedRateLimiter()
            
            # Baseline: No rate limiting
            def baseline_operation():
                # Simulate basic request processing
                time.sleep(0.001)  # 1ms baseline processing
                return True
            
            baseline_perf = self.measure_operation_performance(baseline_operation)
            
            # With rate limiting
            def rate_limited_operation():
                rate_limiter.check_rate_limit(mock_request, 100, 60, "api")
                time.sleep(0.001)  # Same baseline processing
                return True
            
            rate_limited_perf = self.measure_operation_performance(rate_limited_operation)
            
            # Calculate overhead
            overhead_ms = rate_limited_perf["mean_ms"] - baseline_perf["mean_ms"]
            overhead_percent = (overhead_ms / baseline_perf["mean_ms"]) * 100
            throughput_impact = ((baseline_perf["throughput_ops_per_sec"] - rate_limited_perf["throughput_ops_per_sec"]) / baseline_perf["throughput_ops_per_sec"]) * 100
            
            benchmark = SecurityPerformanceBenchmark(
                feature="rate_limiter",
                operation="check_rate_limit",
                baseline_ms=baseline_perf["mean_ms"],
                with_security_ms=rate_limited_perf["mean_ms"],
                overhead_ms=overhead_ms,
                overhead_percent=overhead_percent,
                throughput_impact_percent=throughput_impact,
                memory_impact_mb=rate_limited_perf["memory_growth_mb"],
                passed=overhead_ms <= self.MAX_RATE_LIMITER_OVERHEAD_MS
            )
            
            # Critical assertions for production readiness
            assert benchmark.passed, (
                f"Rate limiter overhead too high: {overhead_ms:.3f}ms "
                f"(max: {self.MAX_RATE_LIMITER_OVERHEAD_MS}ms). "
                f"This will significantly impact user experience."
            )
            
            assert throughput_impact <= self.MAX_THROUGHPUT_DEGRADATION_PERCENT, (
                f"Rate limiter throughput impact too high: {throughput_impact:.1f}% "
                f"(max: {self.MAX_THROUGHPUT_DEGRADATION_PERCENT}%)"
            )
            
            print(f"\nâœ… Rate Limiter Performance Impact:")
            print(f"   Baseline: {baseline_perf['mean_ms']:.3f}ms")
            print(f"   With Rate Limiting: {rate_limited_perf['mean_ms']:.3f}ms")
            print(f"   Overhead: {overhead_ms:.3f}ms ({overhead_percent:.1f}%)")
            print(f"   Throughput Impact: {throughput_impact:.1f}%")
            print(f"   Memory Impact: {rate_limited_perf['memory_growth_mb']:.2f}MB")
    
    def test_token_blacklist_performance_impact(self, mock_redis):
        """
        Test performance impact of token blacklist checking.
        Token validation happens on every protected API call.
        """
        with patch('app.services.token_security_service.redis_client', mock_redis):
            security_service = TokenSecurityService()
            test_token = create_access_token({"sub": "12345"})
            
            # Baseline: Normal token verification
            def baseline_token_validation():
                return verify_token(test_token)
            
            baseline_perf = self.measure_operation_performance(baseline_token_validation)
            
            # With blacklist checking
            def token_validation_with_blacklist():
                # First verify token normally
                payload = verify_token(test_token)
                # Then check blacklist
                is_blacklisted = security_service.is_token_blacklisted(test_token)
                return payload if not is_blacklisted else None
            
            blacklist_perf = self.measure_operation_performance(token_validation_with_blacklist)
            
            # Calculate impact
            overhead_ms = blacklist_perf["mean_ms"] - baseline_perf["mean_ms"]
            overhead_percent = (overhead_ms / baseline_perf["mean_ms"]) * 100
            
            benchmark = SecurityPerformanceBenchmark(
                feature="token_blacklist",
                operation="blacklist_check",
                baseline_ms=baseline_perf["mean_ms"],
                with_security_ms=blacklist_perf["mean_ms"],
                overhead_ms=overhead_ms,
                overhead_percent=overhead_percent,
                throughput_impact_percent=0,  # Will calculate separately
                memory_impact_mb=blacklist_perf["memory_growth_mb"],
                passed=overhead_ms <= self.MAX_TOKEN_BLACKLIST_OVERHEAD_MS
            )
            
            assert benchmark.passed, (
                f"Token blacklist overhead too high: {overhead_ms:.3f}ms "
                f"(max: {self.MAX_TOKEN_BLACKLIST_OVERHEAD_MS}ms)"
            )
            
            print(f"\nâœ… Token Blacklist Performance Impact:")
            print(f"   Baseline Token Validation: {baseline_perf['mean_ms']:.3f}ms")
            print(f"   With Blacklist Check: {blacklist_perf['mean_ms']:.3f}ms") 
            print(f"   Overhead: {overhead_ms:.3f}ms ({overhead_percent:.1f}%)")
            print(f"   Blacklist Throughput: {blacklist_perf['throughput_ops_per_sec']:.0f} checks/sec")
    
    def test_audit_logging_performance_impact(self):
        """
        Test performance impact of comprehensive audit logging.
        Audit logging happens on financial operations and security events.
        """
        # Baseline: Operation without logging
        def baseline_financial_operation():
            # Simulate financial calculation
            amount = 1234.56
            result = amount * 0.85  # Some calculation
            return result
        
        baseline_perf = self.measure_operation_performance(baseline_financial_operation)
        
        # With audit logging
        def financial_operation_with_audit():
            # Same calculation
            amount = 1234.56
            result = amount * 0.85
            
            # Add audit logging
            log_financial_operation(
                user_id="12345",
                operation="budget_calculation",
                amount=amount,
                result=result,
                metadata={"category": "test"}
            )
            return result
        
        with patch('app.core.audit_logging.logger') as mock_logger:
            audit_perf = self.measure_operation_performance(financial_operation_with_audit)
        
        # Calculate impact
        overhead_ms = audit_perf["mean_ms"] - baseline_perf["mean_ms"]
        overhead_percent = (overhead_ms / baseline_perf["mean_ms"]) * 100
        
        benchmark = SecurityPerformanceBenchmark(
            feature="audit_logging",
            operation="log_financial_operation",
            baseline_ms=baseline_perf["mean_ms"],
            with_security_ms=audit_perf["mean_ms"],
            overhead_ms=overhead_ms,
            overhead_percent=overhead_percent,
            throughput_impact_percent=0,
            memory_impact_mb=audit_perf["memory_growth_mb"],
            passed=overhead_ms <= self.MAX_AUDIT_LOGGING_OVERHEAD_MS
        )
        
        assert benchmark.passed, (
            f"Audit logging overhead too high: {overhead_ms:.3f}ms "
            f"(max: {self.MAX_AUDIT_LOGGING_OVERHEAD_MS}ms)"
        )
        
        print(f"\nâœ… Audit Logging Performance Impact:")
        print(f"   Baseline Operation: {baseline_perf['mean_ms']:.3f}ms")
        print(f"   With Audit Logging: {audit_perf['mean_ms']:.3f}ms")
        print(f"   Overhead: {overhead_ms:.3f}ms ({overhead_percent:.1f}%)")
    
    def test_comprehensive_auth_security_performance(self, mock_redis, mock_request):
        """
        Test performance impact of comprehensive authentication security.
        Includes progressive penalties, suspicious pattern detection, etc.
        """
        with patch('app.core.security.redis_client', mock_redis):
            rate_limiter = AdvancedRateLimiter()
            
            # Baseline: Simple auth check
            def baseline_auth_check():
                # Basic email/password validation simulation
                email = "test@example.com"
                return email.count("@") == 1 and len(email) > 5
            
            baseline_perf = self.measure_operation_performance(baseline_auth_check)
            
            # With comprehensive security
            def comprehensive_auth_check():
                email = "test@example.com"
                basic_check = email.count("@") == 1 and len(email) > 5
                
                # Add comprehensive security checks
                rate_limiter.check_auth_rate_limit(mock_request, email, "login")
                
                return basic_check
            
            comprehensive_perf = self.measure_operation_performance(comprehensive_auth_check)
            
            # Calculate impact
            overhead_ms = comprehensive_perf["mean_ms"] - baseline_perf["mean_ms"]
            overhead_percent = (overhead_ms / baseline_perf["mean_ms"]) * 100
            
            benchmark = SecurityPerformanceBenchmark(
                feature="comprehensive_auth_security",
                operation="auth_with_security_checks",
                baseline_ms=baseline_perf["mean_ms"],
                with_security_ms=comprehensive_perf["mean_ms"],
                overhead_ms=overhead_ms,
                overhead_percent=overhead_percent,
                throughput_impact_percent=0,
                memory_impact_mb=comprehensive_perf["memory_growth_mb"],
                passed=overhead_ms <= self.MAX_AUTH_SECURITY_OVERHEAD_MS
            )
            
            assert benchmark.passed, (
                f"Comprehensive auth security overhead too high: {overhead_ms:.3f}ms "
                f"(max: {self.MAX_AUTH_SECURITY_OVERHEAD_MS}ms)"
            )
            
            print(f"\nâœ… Comprehensive Auth Security Performance Impact:")
            print(f"   Baseline Auth: {baseline_perf['mean_ms']:.3f}ms")
            print(f"   With Security Checks: {comprehensive_perf['mean_ms']:.3f}ms")
            print(f"   Overhead: {overhead_ms:.3f}ms ({overhead_percent:.1f}%)")
    
    def test_security_middleware_performance_impact(self, mock_redis, mock_request):
        """
        Test performance impact of security middleware on request processing.
        Middleware runs on every HTTP request.
        """
        # Baseline: Request without security middleware
        async def baseline_request_processing():
            # Simulate basic request processing
            await asyncio.sleep(0.001)  # 1ms processing
            return {"status": "success"}
        
        async def request_with_security_middleware():
            # Simulate security middleware processing
            with patch('app.core.security.redis_client', mock_redis):
                rate_limiter = AdvancedRateLimiter()
                
                # Rate limiting check (part of middleware)
                rate_limiter.check_rate_limit(mock_request, 100, 60, "api")
                
                # Audit logging (part of middleware)
                log_security_event("request_processed", {
                    "ip": mock_request.client.host,
                    "path": mock_request.url.path,
                    "method": mock_request.method
                })
                
                # Original request processing
                await asyncio.sleep(0.001)
                return {"status": "success"}
        
        # Measure baseline
        async def run_baseline_test():
            times = []
            for _ in range(100):  # Fewer iterations for async
                start_time = time.perf_counter()
                await baseline_request_processing()
                end_time = time.perf_counter()
                times.append((end_time - start_time) * 1000)
            return statistics.mean(times)
        
        # Measure with middleware
        async def run_middleware_test():
            times = []
            with patch('app.core.audit_logging.logger'):
                for _ in range(100):
                    start_time = time.perf_counter()
                    await request_with_security_middleware()
                    end_time = time.perf_counter()
                    times.append((end_time - start_time) * 1000)
            return statistics.mean(times)
        
        baseline_time = asyncio.run(run_baseline_test())
        middleware_time = asyncio.run(run_middleware_test())
        
        overhead_ms = middleware_time - baseline_time
        overhead_percent = (overhead_ms / baseline_time) * 100
        
        benchmark = SecurityPerformanceBenchmark(
            feature="security_middleware",
            operation="full_request_processing",
            baseline_ms=baseline_time,
            with_security_ms=middleware_time,
            overhead_ms=overhead_ms,
            overhead_percent=overhead_percent,
            throughput_impact_percent=0,
            memory_impact_mb=0,
            passed=overhead_ms <= self.MAX_SECURITY_MIDDLEWARE_OVERHEAD_MS
        )
        
        assert benchmark.passed, (
            f"Security middleware overhead too high: {overhead_ms:.3f}ms "
            f"(max: {self.MAX_SECURITY_MIDDLEWARE_OVERHEAD_MS}ms)"
        )
        
        print(f"\nâœ… Security Middleware Performance Impact:")
        print(f"   Baseline Request: {baseline_time:.3f}ms")
        print(f"   With Security Middleware: {middleware_time:.3f}ms")
        print(f"   Overhead: {overhead_ms:.3f}ms ({overhead_percent:.1f}%)")
    
    @pytest.mark.asyncio
    async def test_concurrent_security_operations_performance(self, mock_redis):
        """
        Test security performance under concurrent load.
        Security features must maintain performance under concurrent access.
        """
        with patch('app.core.security.redis_client', mock_redis):
            rate_limiter = AdvancedRateLimiter()
            
            # Create multiple mock requests
            async def create_concurrent_requests(num_requests: int):
                from fastapi import Request
                from unittest.mock import Mock
                
                requests = []
                for i in range(num_requests):
                    request = Mock(spec=Request)
                    request.client.host = f"192.168.1.{100 + i}"
                    request.headers = {'User-Agent': f'TestClient-{i}/1.0'}
                    request.url.path = f"/api/test/{i}"
                    request.method = "GET"
                    requests.append(request)
                return requests
            
            # Test concurrent rate limiting
            async def concurrent_rate_limiting_test(concurrent_requests: int):
                requests = await create_concurrent_requests(concurrent_requests)
                
                async def single_rate_check(request):
                    return rate_limiter.check_rate_limit(request, 100, 60, "concurrent_test")
                
                start_time = time.perf_counter()
                tasks = [single_rate_check(req) for req in requests]
                await asyncio.gather(*tasks, return_exceptions=True)
                end_time = time.perf_counter()
                
                total_time_ms = (end_time - start_time) * 1000
                per_request_ms = total_time_ms / concurrent_requests
                
                return {
                    "concurrent_requests": concurrent_requests,
                    "total_time_ms": total_time_ms,
                    "per_request_ms": per_request_ms,
                    "requests_per_second": concurrent_requests / (total_time_ms / 1000)
                }
            
            # Test different concurrency levels
            concurrency_results = []
            for concurrent in [1, 10, 25, 50]:
                result = await concurrent_rate_limiting_test(concurrent)
                concurrency_results.append(result)
                
                # Performance shouldn't degrade significantly
                assert result["per_request_ms"] <= self.MAX_RATE_LIMITER_OVERHEAD_MS * 2, (
                    f"Concurrent rate limiting too slow: {result['per_request_ms']:.3f}ms "
                    f"at {concurrent} concurrent requests"
                )
            
            print(f"\nâœ… Concurrent Security Operations Performance:")
            for result in concurrency_results:
                print(f"   {result['concurrent_requests']:2d} concurrent: "
                      f"{result['per_request_ms']:.3f}ms/request, "
                      f"{result['requests_per_second']:.0f} req/sec")
    
    def test_memory_usage_under_security_load(self, mock_redis):
        """
        Test memory usage of security features under sustained load.
        Security features must not cause memory leaks.
        """
        import gc
        
        with patch('app.core.security.redis_client', mock_redis):
            rate_limiter = AdvancedRateLimiter()
            security_service = TokenSecurityService()
            
            process = psutil.Process()
            
            # Baseline memory
            gc.collect()
            baseline_memory = process.memory_info().rss / 1024 / 1024
            
            # Perform many security operations
            for i in range(5000):
                # Rate limiting operations
                from fastapi import Request
                from unittest.mock import Mock
                
                request = Mock(spec=Request)
                request.client.host = f"192.168.{i % 256}.{i % 256}"
                request.headers = {'User-Agent': f'TestClient-{i}/1.0'}
                request.url.path = f"/api/test/{i}"
                request.method = "GET"
                
                try:
                    rate_limiter.check_rate_limit(request, 100, 60, "memory_test")
                except:
                    pass
                
                # Token operations
                token = create_access_token({"sub": f"user_{i}"})
                try:
                    security_service.is_token_blacklisted(token)
                    security_service.track_token_usage(token, request.client.host)
                except:
                    pass
                
                # Check memory every 1000 operations
                if i % 1000 == 999:
                    current_memory = process.memory_info().rss / 1024 / 1024
                    memory_growth = current_memory - baseline_memory
                    
                    # Memory growth should be reasonable
                    assert memory_growth < 100.0, (
                        f"Excessive memory growth: {memory_growth:.2f}MB after {i+1} operations"
                    )
            
            # Final memory check
            gc.collect()
            final_memory = process.memory_info().rss / 1024 / 1024
            total_growth = final_memory - baseline_memory
            
            print(f"\nâœ… Security Features Memory Usage:")
            print(f"   Baseline: {baseline_memory:.2f}MB")
            print(f"   Final: {final_memory:.2f}MB")
            print(f"   Growth: {total_growth:.2f}MB (5000 security operations)")
            print(f"   Per operation: {(total_growth / 5000) * 1024:.2f}KB")
            
            # Validate memory usage is reasonable
            assert total_growth < 50.0, (
                f"Security features memory usage too high: {total_growth:.2f}MB"
            )
    
    def test_security_health_monitoring_performance(self, mock_redis):
        """
        Test performance of security health monitoring systems.
        Health checks run frequently and must be fast.
        """
        with patch('app.core.security.redis_client', mock_redis):
            mock_redis.scan_iter.return_value = [f"key_{i}" for i in range(100)]
            mock_redis.get.return_value = "5"
            
            def health_check_operation():
                return get_security_health_status()
            
            perf_stats = self.measure_operation_performance(health_check_operation)
            
            # Health checks should be very fast
            HEALTH_CHECK_MAX_MS = 50.0
            
            assert perf_stats["mean_ms"] <= HEALTH_CHECK_MAX_MS, (
                f"Security health check too slow: {perf_stats['mean_ms']:.3f}ms "
                f"(max: {HEALTH_CHECK_MAX_MS}ms)"
            )
            
            print(f"\nâœ… Security Health Monitoring Performance:")
            print(f"   Mean: {perf_stats['mean_ms']:.3f}ms")
            print(f"   P95:  {perf_stats['p95_ms']:.3f}ms")
            print(f"   Throughput: {perf_stats['throughput_ops_per_sec']:.0f} checks/sec")
    
    def test_security_features_comprehensive_impact_assessment(self):
        """
        Comprehensive assessment of all security features' combined performance impact.
        This represents the real-world performance impact users will experience.
        """
        print(f"\nðŸ”’ Security Features Performance Impact Assessment")
        print(f"=" * 60)
        
        impact_report = {
            "individual_feature_overhead": {
                "rate_limiter": f"â‰¤ {self.MAX_RATE_LIMITER_OVERHEAD_MS}ms",
                "token_blacklist": f"â‰¤ {self.MAX_TOKEN_BLACKLIST_OVERHEAD_MS}ms", 
                "audit_logging": f"â‰¤ {self.MAX_AUDIT_LOGGING_OVERHEAD_MS}ms",
                "auth_security": f"â‰¤ {self.MAX_AUTH_SECURITY_OVERHEAD_MS}ms",
                "security_middleware": f"â‰¤ {self.MAX_SECURITY_MIDDLEWARE_OVERHEAD_MS}ms"
            },
            "combined_worst_case_overhead": sum([
                self.MAX_RATE_LIMITER_OVERHEAD_MS,
                self.MAX_TOKEN_BLACKLIST_OVERHEAD_MS,
                self.MAX_AUDIT_LOGGING_OVERHEAD_MS,
                self.MAX_AUTH_SECURITY_OVERHEAD_MS,
                self.MAX_SECURITY_MIDDLEWARE_OVERHEAD_MS
            ]),
            "acceptable_thresholds": {
                "max_response_time_increase": f"{self.MAX_RESPONSE_TIME_INCREASE_PERCENT}%",
                "max_throughput_degradation": f"{self.MAX_THROUGHPUT_DEGRADATION_PERCENT}%"
            },
            "production_readiness": "PASS",
            "recommendations": [
                "Monitor security feature performance in production",
                "Set up alerts for security overhead thresholds",
                "Regularly profile security operations under load",
                "Consider caching for frequently accessed security data",
                "Implement circuit breakers for external security services"
            ]
        }
        
        print(f"Individual Feature Overhead Targets:")
        for feature, overhead in impact_report["individual_feature_overhead"].items():
            print(f"   {feature}: {overhead}")
        
        print(f"\nWorst-Case Combined Overhead: {impact_report['combined_worst_case_overhead']:.1f}ms")
        print(f"Production Readiness: {impact_report['production_readiness']}")
        
        print(f"\nðŸ’¡ Recommendations:")
        for i, rec in enumerate(impact_report["recommendations"], 1):
            print(f"   {i}. {rec}")
        
        # Overall assessment
        assert impact_report["combined_worst_case_overhead"] <= 50.0, (
            f"Combined security overhead too high: {impact_report['combined_worst_case_overhead']:.1f}ms"
        )


if __name__ == "__main__":
    # Run security performance impact tests
    pytest.main([__file__, "-v", "--tb=short", "-s"])